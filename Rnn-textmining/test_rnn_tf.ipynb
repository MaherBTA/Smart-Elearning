{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import collections\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordModel:\n",
    "    \n",
    "    def __init__(self, batch_size, dimension_size, learning_rate, vocabulary_size):\n",
    "        \n",
    "        self.train_inputs = tf.placeholder(tf.int32, shape = [batch_size])\n",
    "        self.train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "        \n",
    "        # randomly generated initial value for each word dimension, between -1.0 to 1.0\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, dimension_size], -1.0, 1.0))\n",
    "        \n",
    "        # find train_inputs from embeddings\n",
    "        embed = tf.nn.embedding_lookup(embeddings, self.train_inputs)\n",
    "        \n",
    "        # estimation for not normalized dataset\n",
    "        self.nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, dimension_size], stddev = 1.0 / np.sqrt(dimension_size)))\n",
    "        \n",
    "        # each node have their own bias\n",
    "        self.nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        # calculate loss from nce, then calculate mean\n",
    "        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights = self.nce_weights, biases = self.nce_biases, labels = self.train_labels,\n",
    "                                                  inputs = embed, num_sampled = batch_size / 2, num_classes = vocabulary_size))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        \n",
    "        # normalize the data by simply reduce sum\n",
    "        self.norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        \n",
    "        # normalizing each embed\n",
    "        self.normalized_embeddings = embeddings / self.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "\n",
    "    dataset = pd.read_csv(filename, sep = ',',header = None ,engine='python')\n",
    "    rows = dataset.shape[0]\n",
    "    \n",
    "    # last column is our target (the first)\n",
    "    label = dataset.ix[:,0].values\n",
    "        \n",
    "    # get the other columns as features\n",
    "    concated = []\n",
    "    data = dataset.ix[:, 1:].values\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        string = \"\"\n",
    "            \n",
    "        for k in range(data.shape[1]):\n",
    "            string += data[i][k] + \" \"\n",
    "            \n",
    "        concated.append(string) \n",
    "    \n",
    "    # get all split strings from second column and second last column\n",
    "    dataset = dataset.ix[:, 1:].values\n",
    "    string = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        for k in range(dataset.shape[1]):\n",
    "            string += dataset[i][k].split()\n",
    "    \n",
    "    return string, concated, label, list(set(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename =  os.getcwd() + \"/dbpedia_data/\" + \"train.csv\"\n",
    "get_data  = read_data(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocabulary_size):\n",
    "    count = []\n",
    "    # extend count\n",
    "    # sorted decending order of words\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        #simply add dictionary of word, used frequently placed top\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "\n",
    "        data.append(index)\n",
    "    \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocabulary_size):\n",
    "    count = []\n",
    "    # extend count\n",
    "    # sorted decending order of words\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        #simply add dictionary of word, used frequently placed top\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "\n",
    "        data.append(index)\n",
    "    \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reverse_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_skipgram(words, batch_size, num_skips, skip_window):\n",
    "    data_index = 0\n",
    "    \n",
    "    #check batch_size able to convert into number of skip in skip-grams method\n",
    "    assert batch_size % num_skips == 0\n",
    "    \n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    # create batch for model input\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    \n",
    "    # a buffer to placed skip-grams sentence\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for i in range(span):\n",
    "        buffer.append(words[data_index])\n",
    "        data_index = (data_index + 1) % len(words)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                # random a word from the sentence\n",
    "                # if random word still a word already chosen, simply keep looping\n",
    "                target = random.randint(0, span - 1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        \n",
    "        buffer.append(words[data_index])\n",
    "        data_index = (data_index + 1) % len(words)\n",
    "    \n",
    "    data_index = (data_index + len(words) - span) % len(words)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generatevector(dimension, batch_size, skip_size, skip_window, num_skips, iteration, words_real):\n",
    "    \n",
    "    print('data size: ', len(words_real))\n",
    "    data, dictionary, reverse_dictionary = build_dataset(words_real, len(words_real))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    print('Creating Word2Vec model.')\n",
    "    \n",
    "    model = WordModel(batch_size, dimension, 0.01, len(dictionary))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    last_time = time.time()\n",
    "    \n",
    "    for step in range(iteration):\n",
    "        new_time = time.time()\n",
    "        batch_inputs, batch_labels = generate_batch_skipgram(data, batch_size, num_skips, skip_window)\n",
    "        feed_dict = {model.train_inputs: batch_inputs, model.train_labels: batch_labels}\n",
    "        \n",
    "        _, loss = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\n",
    "        \n",
    "        if ((step + 1) % 1000) == 0:\n",
    "            print('epoch: ', step + 1, ', loss: ', loss, ', speed: ', time.time() - new_time)\n",
    "    \n",
    "    tf.reset_default_graph()       \n",
    "    return dictionary, reverse_dictionary, model.normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimension = 32\n",
    "skip_size = 8\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "iteration_train_vectors = 5000\n",
    "\n",
    "num_layers = 2\n",
    "size_layer = 256\n",
    "learning_rate = 0.001\n",
    "epoch = 10\n",
    "batch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:  252928\n",
      "Creating Word2Vec model.\n",
      "epoch:  1000 , loss:  21.9267 , speed:  0.00928807258606\n",
      "epoch:  2000 , loss:  30.0589 , speed:  0.0093240737915\n",
      "epoch:  3000 , loss:  10.31 , speed:  0.00866293907166\n",
      "epoch:  4000 , loss:  11.2961 , speed:  0.00854802131653\n",
      "epoch:  5000 , loss:  14.5233 , speed:  0.00874209403992\n"
     ]
    }
   ],
   "source": [
    "filename =  os.getcwd() + \"/dbpedia_data/\" + \"train.csv\"\n",
    "string, data, label, vocab =  read_data(filename)\n",
    "label_encode = LabelEncoder().fit_transform(label)\n",
    "dictionary, reverse_dictionary, vectors = generatevector(dimension, dimension, skip_size, skip_window, num_skips, iteration_train_vectors, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, num_layers, size_layer, dimension_input, dimension_output, learning_rate):\n",
    "        \n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.LSTMCell(size_layer, activation = tf.nn.relu)\n",
    "        \n",
    "        self.rnn_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        \n",
    "        # [dimension of word, batch size, dimension input]\n",
    "        self.X = tf.placeholder(tf.float32, [None, None, dimension_input])\n",
    "        \n",
    "        #[batch size, dimension input]\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        \n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(self.rnn_cells, self.X, dtype = tf.float32)\n",
    "        \n",
    "        self.rnn_W = tf.Variable(tf.random_normal((size_layer, dimension_output)))\n",
    "        self.rnn_B = tf.Variable(tf.random_normal([dimension_output]))\n",
    "        \n",
    "        self.logits = tf.matmul(self.outputs[-1], self.rnn_W) + self.rnn_B\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        \n",
    "        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda2_410/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, label_encode, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimension_input = len(vocab)\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(num_layers, size_layer, dimension_input, np.unique(label_encode).shape[0], learning_rate)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company',\n",
       " 'Educational Institution',\n",
       " 'Artist',\n",
       " 'Athlete',\n",
       " 'Office Holder',\n",
       " 'MeanOfTransportation',\n",
       " 'Building',\n",
       " 'NaturalPlace',\n",
       " 'Village',\n",
       " 'Animal',\n",
       " 'Plant',\n",
       " 'Album',\n",
       " 'Film',\n",
       " 'WrittenWork']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes = pd.read_fwf('dbpedia_data/classes.txt',sep = '\\n',header = None)\n",
    "lables = list(df_classes[0])\n",
    "lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4001 , loss:  2.43476504507 , accuracy train:  0.227699536778 s / batch:  7.06298863384\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "ACC_TRAIN, ACC_TEST, LOST = [], [], []\n",
    "#for i in range(epoch):\n",
    "i = 4000\n",
    "total_cost = 0\n",
    "total_accuracy = 0\n",
    "last_time = time.time()\n",
    "    \n",
    "for n in range(0, (len(X_train) // batch) * batch, batch):\n",
    "    batch_x = np.zeros((dimension, batch, dimension_input))\n",
    "    batch_y = np.zeros((batch, np.unique(Y_train).shape[0]))\n",
    "    for k in range(batch):\n",
    "        emb_data = np.zeros((dimension, dimension_input), dtype = np.float32)\n",
    "        for _, text in enumerate(X_train[n + k].split()):\n",
    "            # if the word got in the vocab\n",
    "            try:\n",
    "                emb_data[:, vocab.index(text)] += vectors[dictionary[text], :]\n",
    "            # if not, skip\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        batch_y[k, int(Y_train[n + k])] = 1.0\n",
    "        batch_x[:, k, :] = emb_data[:, :]\n",
    "    loss, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "    total_accuracy += sess.run(model.accuracy, feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "    total_cost += loss\n",
    "        \n",
    "total_cost /= (len(X_train) // batch)\n",
    "total_accuracy /= (len(X_train) // batch)\n",
    "times = (time.time() - last_time) / (len(X_train) // batch)\n",
    "        \n",
    "ACC_TRAIN.append(total_accuracy)\n",
    "LOST.append(total_cost)\n",
    "        \n",
    "print('epoch: ', i + 1, ', loss: ', total_cost, ', accuracy train: ', total_accuracy, 's / batch: ', times)\n",
    "        \n",
    "batch_x = np.zeros((dimension, Y_test.shape[0], dimension_input))\n",
    "batch_y = np.zeros((Y_test.shape[0], np.unique(Y_test).shape[0]))\n",
    "    \n",
    "for k in range(10):#Y_test.shape[0]):\n",
    "    emb_data = np.zeros((dimension, dimension_input), dtype = np.float32)\n",
    "    for _, text in enumerate(X_test[k].split()):\n",
    "        # if the word got in the vocab\n",
    "        try:\n",
    "            emb_data[:, vocab.index(text)] += vectors[dictionary[text], :]\n",
    "        # if not, skip\n",
    "        except:\n",
    "            continue\n",
    "                \n",
    "    batch_y[k, int(Y_test[k])] = 1.0 \n",
    "    batch_x[:, k, :] = emb_data[:, :]\n",
    "            \n",
    "testing_acc, logits = sess.run([model.accuracy, tf.cast(tf.argmax(model.logits, 1), tf.int32)], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "print ('testing accuracy: ', testing_acc)\n",
    "ACC_TEST.append(testing_acc)\n",
    "print (metrics.classification_report(Y_test, logits, target_names = lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x = np.zeros((dimension, Y_test.shape[0], dimension_input))\n",
    "batch_y = np.zeros((Y_test.shape[0], np.unique(Y_test).shape[0]))\n",
    "    \n",
    "for k in range(10):#Y_test.shape[0]):\n",
    "    emb_data = np.zeros((dimension, dimension_input), dtype = np.float32)\n",
    "    for _, text in enumerate(X_test[k].split()):\n",
    "        # if the word got in the vocab\n",
    "        try:\n",
    "            emb_data[:, vocab.index(text)] += vectors[dictionary[text], :]\n",
    "        # if not, skip\n",
    "        except:\n",
    "            continue\n",
    "                \n",
    "    batch_y[k, int(Y_test[k])] = 1.0 \n",
    "    batch_x[:, k, :] = emb_data[:, :]\n",
    "            \n",
    "testing_acc, logits = sess.run([model.accuracy, tf.cast(tf.argmax(model.logits, 1), tf.int32)], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "print ('testing accuracy: ', testing_acc)\n",
    "ACC_TEST.append(testing_acc)\n",
    "print (metrics.classification_report(Y_test, logits, target_names = lables))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
