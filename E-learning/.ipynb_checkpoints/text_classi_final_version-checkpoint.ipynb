{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import io\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os,fnmatch\n",
    "import re\n",
    "import matplotlib as plt\n",
    "import itertools\n",
    "import codecs\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/nbuser/nltk_data/tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f6e51f2bb8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download nltk.corpus firstly ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/nbuser/nltk_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/nbuser/nltk_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/nbuser/nltk_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/nbuser/nltk_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    668\u001b[0m                                     subsequent_indent=prefix+prefix2+' '*4))\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Handle Packages (delegate to a helper function).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_download_package\u001b[0;34m(self, info, download_dir, force)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Download the file.  This will raise an IOError if the url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/nbuser/nltk_data/tokenizers'"
     ]
    }
   ],
   "source": [
    "# Download nltk.corpus firstly , \n",
    "nltk.download(info_or_id='punkt', download_dir='/home/sneaky/nltk_data')\n",
    "nltk.download(info_or_id='averaged_perceptron_tagger', download_dir='/home/sneaky/nltk_data')\n",
    "nltk.download(info_or_id='stopwords', download_dir='/home/sneaky/nltk_data')\n",
    "nltk.download(info_or_id='wordnet', download_dir='/home/sneaky/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import math\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_content(path,filtere):\n",
    "    lst = sorted(fnmatch.filter(os.listdir(path), filtere))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(sentances,noise_list):\n",
    "    sent_list = []\n",
    "    for sent in sentances:\n",
    "        words = sent.split() \n",
    "        noise_free_words = [word for word in words if word not in noise_list] \n",
    "        noise_free_text = \" \".join(noise_free_words) \n",
    "  #      print(noise_free_text)\n",
    "        sent_list = sent_list + [noise_free_text]\n",
    "    return sent_list\n",
    "\n",
    "def stem_sent(sentances):\n",
    "    sent_list = []\n",
    "    lem = WordNetLemmatizer()\n",
    "#    stem = PorterStemmer()\n",
    "    for sent in sentances:\n",
    "        words = sent.split()       \n",
    "        words_stemed = [lem.lemmatize(word, \"v\") for word in words]\n",
    "        stemmed_text  = \" \".join(words_stemed)\n",
    "        sent_list = sent_list + [stemmed_text]\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text,noise_list):\n",
    "    text = text.replace('..', '.')\n",
    "#   text = text.replace('?', '')\n",
    "    sentances = sent_tokenize(text)\n",
    "    # Remove Punctionation\n",
    " #   exclude = set(string.punctuation)\n",
    " #   sentances = [''.join(ch for ch in sent if ch not in exclude) for sent in sentances]\n",
    "\n",
    "#-------------- Noise removal -----------------------------------###\n",
    "    sentances = remove_stop(sentances,noise_list)\n",
    "    sentances = stem_sent(sentances)\n",
    "    return sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_by_folder(path,noise_list,ls_classes,lst):\n",
    "    corpus = []\n",
    "    if lst is None : # Get all the folder content\n",
    "        lst = get_folder_content(path,'*.txt')\n",
    " \n",
    "    for i in range(len(lst)):\n",
    "        with codecs.open(path+'/'+ lst[i],encoding='utf-8') as shakes:\n",
    "            text = shakes.read()\n",
    "        # If the sentance contains two .. , we must suppress one to improve the spliting output\n",
    "        sentances = cleaning_text(text,noise_list)\n",
    "\n",
    "##### --------- Add the class label to sent ---------------------###\n",
    "        corpus = corpus + [(sent,ls_classes[i]) for sent in sentances]\n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qcm_by_course(df1,course_id):\n",
    "    df2 = df1[df1['Course'] == course_id]\n",
    "    ch = \"\"\n",
    "    if(list(df2.iterrows()) != []):\n",
    "        for index, row in df2.iterrows():\n",
    "            if (row['Type'] == 'tf'):\n",
    "                ch1 = \"\\n\"+ row['Question_text']\n",
    "            else : \n",
    "                if(str(row['Responses_list']) not in [\"nan\",\" \"]):\n",
    "                    ch1 =  \"\\n\"+ row['Question_text']+\"\\n\"+ row['Responses_list']+\"\\n\"\n",
    "                    ch = ch + ch1\n",
    "    return ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_by_file(df,noise_list,ls_classes,lst_course):\n",
    "    corpus = []\n",
    "    \n",
    "#    lst_course = ['CH1C1','CH1C2','CH1C3','CH1C4','CH1C5','CH1C6','CH1C7']\n",
    "    for i in range(len(lst_course)):\n",
    "        course_id = lst_course[i]\n",
    "        text = get_qcm_by_course(df,course_id)\n",
    "        if (text != \"\"):\n",
    "#-------------- Noise removal -----------------------------------###\n",
    "            sentances = cleaning_text(text,noise_list)\n",
    "\n",
    "##### --------- Add the class label to sent ---------------------###\n",
    "            corpus = corpus + [(sent,ls_classes[i]) for sent in sentances]\n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exelfile_to_df(file_name):   \n",
    "    xl_file = pd.ExcelFile(file_name)\n",
    "    dfs = {sheet_name: xl_file.parse(sheet_name) \n",
    "          for sheet_name in xl_file.sheet_names}\n",
    "    return dfs\n",
    "def get_sheet_names(dfs):\n",
    "    return list(dfs)   \n",
    "def get_df_by_sheet(dfs,sheet_name):\n",
    "    df = dfs[sheet_name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDfbyfolder(folder_name,task):\n",
    "    if (task == 'train'):\n",
    "        fname = \"data_train_qcm.xlsx\"\n",
    "    elif (task == 'test'):\n",
    "        fname = \"data_test_qcm.xlsx\"\n",
    "    \n",
    "    if folder_name == \"\":\n",
    "        file_name = os.getcwd() + \"/\" + \"qcm\" + \"/\" + fname\n",
    "    else : \n",
    "        file_name = os.getcwd() + \"/\" + folder_name + \"/\" + \"qcm\" + \"/\" + fname\n",
    "    dfs = Exelfile_to_df(file_name)  ## Import the Excel file of classes\n",
    "    ls_sheets = get_sheet_names(dfs) ## \n",
    "    frames = []\n",
    "    if (ls_sheets[-1] == \"classes\"):\n",
    "        for sh in ls_sheets[:-1]:\n",
    "            df = dfs[sh]\n",
    "            df = df[df.Question_text.notnull()]   \n",
    "            frames.append(df)\n",
    "        df_classes = get_df_by_sheet(dfs,\"classes\")\n",
    "    else:\n",
    "        for sh in ls_sheets:\n",
    "            df = dfs[sh]\n",
    "            df = df[df.Question_text.notnull()]   \n",
    "            frames.append(df)\n",
    "        df_classes = None   \n",
    "    df_courses = pd.concat(frames,ignore_index = True)#get_df_by_sheet(dfs,ls_sheets[0])  # Get Only the first Sheet\n",
    "    return [df_classes,df_courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_corpus(df_classes,df_courses):\n",
    "# Import class list from .txt file \n",
    "# Import Corpus of texts\n",
    "# Import Corpus of QCM\n",
    "       \n",
    "    ### ----- Import class names from Excel file -----###\n",
    "    ls_classes = list(df_classes['class_name']) \n",
    "    lst_text = list(df_classes['text_file'])\n",
    "    lst_qcm = list(df_classes['qcm_file'])\n",
    "    lst_courses = list(df_classes['class_id'])\n",
    "    \n",
    "    ### --------------------------------------------------\n",
    "    \n",
    "    ###--------- Import the list of stopwords ------###\n",
    "    Stop =  stopwords.words('english')\n",
    "    ### ------------ Load Texts --------------------###\n",
    "    path = path = os.getcwd() + \"/\" + \"textes\"    \n",
    "    train_corpus = get_token_by_folder(path,Stop,ls_classes,lst = lst_text)\n",
    "    ### ------------ Load QCM Texts ---------------- ###\n",
    "    ### Load from .txt files\n",
    "    path1 = os.getcwd() + \"/\" + \"qcm\"\n",
    "#    test_corpus = get_token_by_folder(path1,Stop,ls_classes,lst = lst_qcm)\n",
    "    ### Load from .xlsx file\n",
    "    test_corpus = get_token_by_file(df_courses,Stop,ls_classes,lst_courses)\n",
    "    # ---------------- End ---------------------------##\n",
    "    ####################################################\n",
    "    return [train_corpus,test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_train(train_corpus,test_corpus):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    for row in train_corpus:\n",
    "        train_data.append(row[0])\n",
    "        train_labels.append(row[1])\n",
    "    test_data = []\n",
    "    test_labels = [] \n",
    "    for row in test_corpus:\n",
    "        test_data.append(row[0]) \n",
    "        test_labels.append(row[1])\n",
    "    return [train_data,train_labels,test_data,test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program of first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install xlrd >= 0.9.0 for Excel support",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'xlrd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1f46b27ebc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### ---------- Main program --------- ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mdf_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_courses\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDfbyfolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_to_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_courses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_to_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f05068d84582>\u001b[0m in \u001b[0;36mgetDfbyfolder\u001b[0;34m(folder_name, task)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"qcm\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExelfile_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## Import the Excel file of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mls_sheets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sheet_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-317fa2c5cda4>\u001b[0m in \u001b[0;36mExelfile_to_df\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mExelfile_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mxl_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     dfs = {sheet_name: xl_file.parse(sheet_name) \n\u001b[1;32m      4\u001b[0m           for sheet_name in xl_file.sheet_names}\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__VERSION__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Install xlrd >= 0.9.0 for Excel support"
     ]
    }
   ],
   "source": [
    "### ---------- Main program --------- ##\n",
    "[df_classes,df_courses] = getDfbyfolder(\"\",'train')\n",
    "[train_corpus,test_corpus] = load_data_to_corpus(df_classes,df_courses)\n",
    "[train_data,train_labels,test_data,test_labels] = corpus_to_train(train_corpus,test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Test Combined features and All Features Caract \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HashingVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8a0e41c67ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### ---- Put here all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdict_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashingVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HashingVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create feature vectors \n",
    "### ---- Put here all features \n",
    "dict_vect = HashingVectorizer()\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "\n",
    "#list_comb = [('tf', vectorizer), ('cnt',count_vect), ('hs',dict_vect) ]\n",
    "list_comb = [('tf', vectorizer),('cnt',count_vect)]\n",
    "combined_f = FeatureUnion(list_comb)\n",
    "# Train the feature vectors\n",
    "#X_train1 = count_vect.fit_transform(train_data)\n",
    "#X_test1 = count_vect.transform(test_data)\n",
    "\n",
    "# -----------\n",
    "\n",
    "prediction = []\n",
    "\n",
    "X_train2 = combined_f.fit_transform(train_data)\n",
    "X_test2 = combined_f.transform(test_data)\n",
    "\n",
    "# ---------------\n",
    "model_svm = svm.SVC(kernel='linear') \n",
    "model_svm.fit(X_train2, train_labels) \n",
    "# -----\n",
    "prediction = model_svm.predict(X_test2)\n",
    "\n",
    "print (classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_df(features_list = ['Question_text','Responses_list'],labels_list = ['level']):\n",
    "    F1= features_list[0]\n",
    "    F2 = features_list[1]\n",
    "    L1 = labels_list[0]\n",
    "    #### ------- Load data from Excel file ---------#####\n",
    "    \n",
    "    #### --------- Get Train Data ------------------#####\n",
    "\n",
    "    [df_classes,df_courses] = getDfbyfolder(\"\",'train')\n",
    "    ####---------\n",
    "    qdf_train = list(df_courses[F1])  # Feature 1\n",
    "\n",
    "    rdf_train = list(df_courses[F2]) # Feature 2\n",
    "\n",
    "    \n",
    "    ### ---------\n",
    "    train_labels = list(df_courses[L1])\n",
    "\n",
    "    #### --------- Get Test Data ------------------#####\n",
    "    [df_N,df_test] = getDfbyfolder(\"\",'test')\n",
    "    ####---------\n",
    "    qdf_test = list(df_test[F1])  # Feature 1\n",
    "    rdf_test = list(df_test[F2]) # Feature 2\n",
    "\n",
    "    ### ---------\n",
    "    test_labels = list(df_test[L1])\n",
    "\n",
    "    train_data = [qdf_train,rdf_train]\n",
    "    test_data = [qdf_test,rdf_test]\n",
    "    return [train_data,train_labels,test_data,test_labels]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install xlrd >= 0.9.0 for Excel support",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'xlrd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e7e930802b87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Question_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Responses_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'level'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Create feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#TfidfTransformer()#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-032017e5fdab>\u001b[0m in \u001b[0;36mload_data_to_df\u001b[0;34m(features_list, labels_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#### --------- Get Train Data ------------------#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mdf_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_courses\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDfbyfolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m####---------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mqdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_courses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Feature 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f05068d84582>\u001b[0m in \u001b[0;36mgetDfbyfolder\u001b[0;34m(folder_name, task)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"qcm\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExelfile_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## Import the Excel file of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mls_sheets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sheet_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-317fa2c5cda4>\u001b[0m in \u001b[0;36mExelfile_to_df\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mExelfile_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mxl_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     dfs = {sheet_name: xl_file.parse(sheet_name) \n\u001b[1;32m      4\u001b[0m           for sheet_name in xl_file.sheet_names}\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__VERSION__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Install xlrd >= 0.9.0 for Excel support"
     ]
    }
   ],
   "source": [
    "[train_data,train_labels,test_data,test_labels] = load_data_to_df(features_list = ['Question_text','Responses_list'],labels_list = ['level'])\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "count_vect = CountVectorizer()#TfidfTransformer()#\n",
    "\n",
    "X_train1 = count_vect.fit_transform(train_data[0])\n",
    "X_test1 = count_vect.transform(test_data[0])\n",
    "\n",
    "X_train2 = count_vect.fit_transform(train_data[1])\n",
    "X_test2 = count_vect.transform(test_data[1])\n",
    "\n",
    "\n",
    "X_train = hstack((X_train1, X_train2))\n",
    "X_test = hstack((X_test1, X_test2))\n",
    "\n",
    "prediction = []\n",
    "\n",
    "model_svm = svm.SVC(kernel='linear') \n",
    "model_svm.fit(X_train, train_labels)\n",
    "\n",
    "#print(X_test1)\n",
    "\n",
    "#print(X_test2.shape[1])\n",
    "prediction = model_svm.predict(X_test)\n",
    "print (classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
