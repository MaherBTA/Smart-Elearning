{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import io\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os,fnmatch\n",
    "import re\n",
    "import matplotlib as plt\n",
    "import itertools\n",
    "import codecs\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nbuser/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/nbuser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk.corpus firstly , \n",
    "nltk.download(info_or_id='punkt', download_dir='/home/nbuser/nltk_data')\n",
    "nltk.download(info_or_id='averaged_perceptron_tagger', download_dir='/home/nbuser/nltk_data')\n",
    "nltk.download(info_or_id='stopwords', download_dir='/home/nbuser/nltk_data')\n",
    "nltk.download(info_or_id='wordnet', download_dir='/home/nbuser/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import math\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_folder_content(path,filtere):\n",
    "    lst = sorted(fnmatch.filter(os.listdir(path), filtere))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop(sentances,noise_list):\n",
    "    sent_list = []\n",
    "    for sent in sentances:\n",
    "        words = sent.split() \n",
    "        noise_free_words = [word for word in words if word not in noise_list] \n",
    "        noise_free_text = \" \".join(noise_free_words) \n",
    "  #      print(noise_free_text)\n",
    "        sent_list = sent_list + [noise_free_text]\n",
    "    return sent_list\n",
    "\n",
    "def stem_sent(sentances):\n",
    "    sent_list = []\n",
    "    lem = WordNetLemmatizer()\n",
    "#    stem = PorterStemmer()\n",
    "    for sent in sentances:\n",
    "        words = sent.split()       \n",
    "        words_stemed = [lem.lemmatize(word, \"v\") for word in words]\n",
    "        stemmed_text  = \" \".join(words_stemed)\n",
    "        sent_list = sent_list + [stemmed_text]\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_text(text,noise_list):\n",
    "    text = text.replace('..', '.')\n",
    "#   text = text.replace('?', '')\n",
    "    sentances = sent_tokenize(text)\n",
    "    # Remove Punctionation\n",
    " #   exclude = set(string.punctuation)\n",
    " #   sentances = [''.join(ch for ch in sent if ch not in exclude) for sent in sentances]\n",
    "\n",
    "#-------------- Noise removal -----------------------------------###\n",
    "    sentances = remove_stop(sentances,noise_list)\n",
    "    sentances = stem_sent(sentances)\n",
    "    return sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_token_by_folder(path,noise_list,ls_classes,lst):\n",
    "    corpus = []\n",
    "    if lst is None : # Get all the folder content\n",
    "        lst = get_folder_content(path,'*.txt')\n",
    " \n",
    "    for i in range(len(lst)):\n",
    "        with codecs.open(path+'/'+ lst[i],encoding='utf-8') as shakes:\n",
    "            text = shakes.read()\n",
    "        # If the sentance contains two .. , we must suppress one to improve the spliting output\n",
    "        sentances = cleaning_text(text,noise_list)\n",
    "\n",
    "##### --------- Add the class label to sent ---------------------###\n",
    "        corpus = corpus + [(sent,ls_classes[i]) for sent in sentances]\n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qcm_by_course(df1,course_id):\n",
    "    df2 = df1[df1['Course'] == course_id]\n",
    "    ch = \"\"\n",
    "    if(list(df2.iterrows()) != []):\n",
    "        for index, row in df2.iterrows():\n",
    "            if (row['Type'] == 'tf'):\n",
    "                ch1 = \"\\n\"+ row['Question_text']\n",
    "            else : \n",
    "                if(str(row['Responses_list']) not in [\"nan\",\" \"]):\n",
    "                    ch1 =  \"\\n\"+ row['Question_text']+\"\\n\"+ row['Responses_list']+\"\\n\"\n",
    "                    ch = ch + ch1\n",
    "    return ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_token_by_file(df,noise_list,ls_classes,lst_course):\n",
    "    corpus = []\n",
    "    \n",
    "#    lst_course = ['CH1C1','CH1C2','CH1C3','CH1C4','CH1C5','CH1C6','CH1C7']\n",
    "    for i in range(len(lst_course)):\n",
    "        course_id = lst_course[i]\n",
    "        text = get_qcm_by_course(df,course_id)\n",
    "        if (text != \"\"):\n",
    "#-------------- Noise removal -----------------------------------###\n",
    "            sentances = cleaning_text(text,noise_list)\n",
    "\n",
    "##### --------- Add the class label to sent ---------------------###\n",
    "            corpus = corpus + [(sent,ls_classes[i]) for sent in sentances]\n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Exelfile_to_df(file_name):   \n",
    "    xl_file = pd.ExcelFile(file_name)\n",
    "    dfs = {sheet_name: xl_file.parse(sheet_name) \n",
    "          for sheet_name in xl_file.sheet_names}\n",
    "    return dfs\n",
    "def get_sheet_names(dfs):\n",
    "    return list(dfs)   \n",
    "def get_df_by_sheet(dfs,sheet_name):\n",
    "    df = dfs[sheet_name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDfbyfolder(folder_name,task):\n",
    "    if (task == 'train'):\n",
    "        fname = \"data_train_qcm.xlsx\"\n",
    "    elif (task == 'test'):\n",
    "        fname = \"data_test_qcm.xlsx\"\n",
    "    \n",
    "    if folder_name == \"\":\n",
    "        file_name = os.getcwd() + \"/\" + \"qcm\" + \"/\" + fname\n",
    "    else : \n",
    "        file_name = os.getcwd() + \"/\" + folder_name + \"/\" + \"qcm\" + \"/\" + fname\n",
    "    dfs = Exelfile_to_df(file_name)  ## Import the Excel file of classes\n",
    "    ls_sheets = get_sheet_names(dfs) ## \n",
    "    frames = []\n",
    "    if (ls_sheets[-1] == \"classes\"):\n",
    "        for sh in ls_sheets[:-1]:\n",
    "            df = dfs[sh]\n",
    "            df = df[df.Question_text.notnull()]   \n",
    "            frames.append(df)\n",
    "        df_classes = get_df_by_sheet(dfs,\"classes\")\n",
    "    else:\n",
    "        for sh in ls_sheets:\n",
    "            df = dfs[sh]\n",
    "            df = df[df.Question_text.notnull()]   \n",
    "            frames.append(df)\n",
    "        df_classes = None   \n",
    "    df_courses = pd.concat(frames,ignore_index = True)#get_df_by_sheet(dfs,ls_sheets[0])  # Get Only the first Sheet\n",
    "    return [df_classes,df_courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_to_corpus(df_classes,df_courses):\n",
    "# Import class list from .txt file \n",
    "# Import Corpus of texts\n",
    "# Import Corpus of QCM\n",
    "       \n",
    "    ### ----- Import class names from Excel file -----###\n",
    "    ls_classes = list(df_classes['class_name']) \n",
    "    lst_text = list(df_classes['text_file'])\n",
    "    lst_qcm = list(df_classes['qcm_file'])\n",
    "    lst_courses = list(df_classes['class_id'])\n",
    "    \n",
    "    ### --------------------------------------------------\n",
    "    \n",
    "    ###--------- Import the list of stopwords ------###\n",
    "    Stop =  stopwords.words('english')\n",
    "    ### ------------ Load Texts --------------------###\n",
    "    path = path = os.getcwd() + \"/\" + \"textes\"    \n",
    "    train_corpus = get_token_by_folder(path,Stop,ls_classes,lst = lst_text)\n",
    "    ### ------------ Load QCM Texts ---------------- ###\n",
    "    ### Load from .txt files\n",
    "    path1 = os.getcwd() + \"/\" + \"qcm\"\n",
    "#    test_corpus = get_token_by_folder(path1,Stop,ls_classes,lst = lst_qcm)\n",
    "    ### Load from .xlsx file\n",
    "    test_corpus = get_token_by_file(df_courses,Stop,ls_classes,lst_courses)\n",
    "    # ---------------- End ---------------------------##\n",
    "    ####################################################\n",
    "    return [train_corpus,test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_to_train(train_corpus,test_corpus):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    for row in train_corpus:\n",
    "        train_data.append(row[0])\n",
    "        train_labels.append(row[1])\n",
    "    test_data = []\n",
    "    test_labels = [] \n",
    "    for row in test_corpus:\n",
    "        test_data.append(row[0]) \n",
    "        test_labels.append(row[1])\n",
    "    return [train_data,train_labels,test_data,test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program of first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ---------- Main program --------- ##\n",
    "[df_classes,df_courses] = getDfbyfolder(\"\",'train')\n",
    "[train_corpus,test_corpus] = load_data_to_corpus(df_classes,df_courses)\n",
    "[train_data,train_labels,test_data,test_labels] = corpus_to_train(train_corpus,test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors \n",
    "### ---- Put here all features \n",
    "dict_vect = HashingVectorizer()\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "#list_comb = [('tf', vectorizer), ('cnt',count_vect), ('hs',dict_vect) ]\n",
    "list_comb = [('tf', vectorizer),('cnt',count_vect)]\n",
    "combined_f = FeatureUnion(list_comb)\n",
    "# Train the feature vectors\n",
    "#X_train1 = count_vect.fit_transform(train_data)\n",
    "#X_test1 = count_vect.transform(test_data)\n",
    "\n",
    "# -----------\n",
    "\n",
    "prediction = []\n",
    "\n",
    "X_train2 = combined_f.fit_transform(train_data)\n",
    "\n",
    "\n",
    "# ---------------\n",
    "model_svm = svm.SVC(kernel='linear') \n",
    "model_svm.fit(X_train2, train_labels) \n",
    "# -----\n",
    "def predict_text(model,input_text):\n",
    "    X_test2 = combined_f.transform([input_text])\n",
    "    prediction = model.predict(X_test2)\n",
    "    return prediction[0]\n",
    "#print (classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radio transmission\n"
     ]
    }
   ],
   "source": [
    "p = predict_text(model_svm,train_data[0])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_to_df(features_list = ['Question_text','Responses_list'],labels_list = ['level']):\n",
    "    F1= features_list[0]\n",
    "    F2 = features_list[1]\n",
    "    L1 = labels_list[0]\n",
    "    #### ------- Load data from Excel file ---------#####\n",
    "    \n",
    "    #### --------- Get Train Data ------------------#####\n",
    "\n",
    "    [df_classes,df_courses] = getDfbyfolder(\"\",'train')\n",
    "    ####---------\n",
    "    train_data = []\n",
    "    qdf_train = list(df_courses[F1])  # Feature 1\n",
    "    rdf_train = list(df_courses[F2]) # Feature 2\n",
    "    for i in range(len(rdf_train)):\n",
    "        train_data.append(qdf_train[i]+rdf_train[i])\n",
    "    \n",
    "    ### ---------\n",
    "    train_labels = list(df_courses[L1])\n",
    "\n",
    "    #### --------- Get Test Data ------------------#####\n",
    "    [df_N,df_test] = getDfbyfolder(\"\",'test')\n",
    "    ####---------\n",
    "    test_data = []\n",
    "    qdf_test = list(df_test[F1])  # Feature 1\n",
    "    rdf_test = list(df_test[F2]) # Feature 2\n",
    "    for i in range(len(rdf_test)):\n",
    "        test_data.append(qdf_test[i]+rdf_test[i])\n",
    "        \n",
    "    ### ---------\n",
    "    test_labels = list(df_test[L1])\n",
    "\n",
    "    return train_data,train_labels,test_data,test_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          2       0.25      0.50      0.33         2\n",
      "          3       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.10      0.20      0.13         5\n",
      "\n",
      "[ 2.  2.  2.  3.  2.]\n"
     ]
    }
   ],
   "source": [
    "train_data,train_labels,test_data,test_labels = load_data_to_df(features_list = ['Question_text','Responses_list'],labels_list = ['level'])\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "count_vect = CountVectorizer()#TfidfTransformer()#\n",
    "\n",
    "X_train = count_vect.fit_transform(train_data)\n",
    "X_test = count_vect.transform(test_data)\n",
    "\n",
    "\n",
    "prediction = []\n",
    "\n",
    "model_svm = svm.SVC(kernel='linear') \n",
    "model_svm.fit(X_train,train_labels)\n",
    "\n",
    "def predict_next(model,input_text):\n",
    "    X_test = count_vect.transform([input_text])\n",
    "    prediction = model.predict(X_test)\n",
    "    return prediction[0]\n",
    "prediction = model_svm.predict(X_test)\n",
    "print (classification_report(test_labels, prediction))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
